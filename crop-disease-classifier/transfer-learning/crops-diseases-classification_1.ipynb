{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning: Using Keras API: Fine-Tuning Pretrained Deep Learning Networks with Data Augmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains Transfer learning code for extracting features using the fine-tuning method of pre-trained  networks. In fine-tuning a pre-trained network, we assume that our custom dataset is small (less than 1000 images per class) and not similar to the dataset that the pre-trained netwoks were trained on (ImageNet dataset in this case). Fine-tuning is complimentary to using a pre-trained model/network for feature extraction. Fine-tuning makes slight adjustments to the abstract representations learned(in the deeper layers) by the pre-trained model to make them more relevant to the custom problem being addressed. In this fine tuning method, we will also use Keras data augmentation methods to randomly transform the original dataset on the fly during model training (which is analogous to generating additional training data). It is advisable to run this code ONLY on a GPU as it is too expensive to run on a CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This procedure is only for the fine tuning process and does not include data pre-processin tasks:\n",
    "\n",
    "1) Load the convolutional base of the pre-trained network/model <br><br>\n",
    "2) Flatten the convolutional base outputs (before feeding them to the densely   \n",
    "  connected classifier <br><br>\n",
    "3) Add a custom densely connected classifier on top of the flattened convolutional base  \n",
    "   model <br><br>\n",
    "4) Freeze the convolutional base model <br><br>\n",
    "5) Compile the model <br><br>\n",
    "6) Train the model (end to end training with data augmentation)--<br>- i.e. train the added part--<br> [Same as feature extraction with data augmentation up to this step] <br> This step trains  the classifier that was added in step 3. You can also save the model after this step to act as the feature extraction part of the model <br><br>\n",
    "7) Unfreeze some layers in the convolutional base of the network (This step is achieved by unfreezing the whole convolutional base followed by the freezing of some individual layers inside the convolutional base) \n",
    " <br><br>\n",
    "  \n",
    "8) Re-compile the model <br><br> It is very important to recompile the model after changing the weight trainability of the layers (unfreezing), otherwise the changes made in step 7 will be ignored\n",
    "  \n",
    "9) Jointly train these (unfrozen) layers with the part added in step 3 <br>\n",
    "\n",
    "Had, at this point, the added custom classifier not been trained in step 6, the error signal propagated through the network during training in this step would be too large, leading to the destruction of the feature representations that were previously learned by the layers being fine-tuned.\n",
    "\n",
    "\n",
    "10) Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Necessary Libraries/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import VGG19\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications import Xception\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions \n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.layers import Input, Flatten, Dense, Dropout\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import merge, Input\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import ImageDataGenerator    # input preprocessing done in the ImageDataGenerator\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle \n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import itertools\n",
    "from imutils import paths\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See data processing code in a separate Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant Variables (Configuration Cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the path to the main input directory of images\n",
    "MAIN_INPUT_DATASET = \"C:\\\\Path\\\\to\\\\Main\\\\Input\\\\Directory\"\n",
    "\n",
    "# initialize the base path to the directory that will contain\n",
    "# the images after computing the training, validation, and testing split\n",
    "BASE_PATH = \"C:\\\\Path\\\\to\\\\Base\\\\Path\"\n",
    "\n",
    "# derive the the paths to the training, validation, and testing directories\n",
    "TRAIN_PATH = os.path.sep.join([BASE_PATH, \"training\"])\n",
    "VAL_PATH = os.path.sep.join([BASE_PATH, \"validation\"])\n",
    "TEST_PATH = os.path.sep.join([BASE_PATH, \"testing\"])\n",
    "\n",
    "# examine whether the derived paths are correct\n",
    "print(TRAIN_PATH)\n",
    "print(VAL_PATH)\n",
    "print(TEST_PATH)\n",
    "\n",
    "# set the percentage of data that will be used for training (training split)\n",
    "TRAIN_SPLIT = 0.8\n",
    " \n",
    "# set the percentage of validation data that will used for validation during training\n",
    "# the validation data set will be a percentage of the training data/i.e it will be split from the training data\n",
    "# here val is set at 25% split from the 80% of training data, i.e. 20% of the total dataset\n",
    "# we end up with train:test:validation splits ratios equal to= 60:20:20\n",
    "VAL_SPLIT = 0.25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure (Training, Validation, and Testing Directory Structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = mpimg.imread(\"C:\\\\Path\\\\to\\\\Directory\\\\Structure\\\\Image\\\\directory_tree.png\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split/Partition the Data into Training, Validation, and Testing Sets\n",
    "\n",
    "See separate code for partitioning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model Training for Crop Diseases Classification\n",
    "\n",
    "The model training code starts from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print out the total number of images in each of the training, validation, \n",
    "# and test splits/directories\n",
    "\n",
    "ntotalTrain = len(list(paths.list_images(TRAIN_PATH)))\n",
    "ntotalVal = len(list(paths.list_images(VAL_PATH)))\n",
    "ntotalTest = len(list(paths.list_images(TEST_PATH)))\n",
    "\n",
    "print('The total number of training images =', ntotalTrain)\n",
    "print('The total number of validation images =', ntotalVal)\n",
    "print('The total number of testing images =', ntotalTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Names of the Pre-Trained Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a list of Keras pretrained model names \n",
    "# the model names to use in this code should be chosen from the following list\n",
    "MODELS = [\"vgg16\", \"vgg19\", \"resnet50\", \"inception\", \"xception\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Keras Pre-Trained Model Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    Name = str(input('Enter the Model you want to run today:...:'))\n",
    "    if Name not in MODELS:\n",
    "        print(\"The model name you entered is not in one of the names in the Models list. Please select one from the list\")\n",
    "        print(\"Please enter one of the following model names: vgg16, vgg19, resnet50, inception, xception\")\n",
    "        # return to the start of the loop\n",
    "        continue \n",
    "    else:\n",
    "        # exit the loop\n",
    "        print(\"You have entered a valid model name\")\n",
    "        break\n",
    "        \n",
    "if Name in (\"vgg16\", \"vgg19\", \"resnet50\"):\n",
    "    input_shape, target_size = (256, 256, 3), (256, 256)\n",
    "      \n",
    "else:\n",
    "    input_shape, target_size = (256, 256, 3), (256, 256)\n",
    "    \n",
    "Network_name = Name                            \n",
    "base = Network_name + \"_conv_base\"\n",
    "\n",
    "# check to see whether the inputs are correct\n",
    "print(\"The name of the chosen Network is...:\", Network_name)\n",
    "print(\"The input shape of the chosen Network is...:\", input_shape)\n",
    "print(\"The target_size of the chosen Network is...:\", target_size)\n",
    "print(\"The convolutional base of the chosen Network is defined as...:\", base)\n",
    "\n",
    "print('{}{}{}{}'.format(\"The network chosen is...: \", Network_name, \", and the conv. base of the Network is...:\", base))    \n",
    "print('{}{}{}{}'.format(\"The target_size for...: \", Network_name, \", is...:\", target_size)) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the batch size\n",
    "batch_size = 30\n",
    "\n",
    "# set the number of epochs\n",
    "epochs = 5\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Data Augmentation \n",
    "<br> The Keras ImageDataGenerator yields batches of images from disk which eliminates <br>\n",
    "the need for holding the entire dataset in memory (good, especially if we are limited in memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Data Generators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the training data augmentation object\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,                                 \n",
    "                            rotation_range=40,                               \n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "\n",
    "# initialize the validation (also used for testing) data augmentation object\n",
    "# Validation and test datasets are not augmented\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255,                                  \n",
    "                                )                                             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Data Generators (Using .flow_from_directory Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the training generator\n",
    "train_generator = train_datagen.flow_from_directory(TRAIN_PATH,\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    target_size=target_size,\n",
    "                                                    color_mode=\"rgb\", \n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=shuffle)\n",
    "\n",
    "# initialize the validation generator\n",
    "val_generator = val_datagen.flow_from_directory(VAL_PATH,\n",
    "                                                class_mode=\"categorical\",\n",
    "                                                target_size=target_size,\n",
    "                                                color_mode=\"rgb\",   \n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=False)  # validation dataset should not be shuffled\n",
    "\n",
    "# initialize the testing generator\n",
    "test_generator = val_datagen.flow_from_directory(TEST_PATH,\n",
    "                                                 class_mode=\"categorical\",  \n",
    "                                                 target_size=target_size,                          \n",
    "                                                 color_mode=\"rgb\",   \n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=False)      # testing dataset should not be shuffled \n",
    "                                                                     # so as to keep data in same order as the labels\n",
    "                                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Imbalanced Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the class weights if the custom dataset is imbalanced with some \n",
    "# classes having more data than others\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "               'balanced',\n",
    "                np.unique(train_generator.classes), \n",
    "                train_generator.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Convolutional Base (of the Pre-trained Model/Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the feature extraction part of the network pre-trained on ImageNet (convolutional base)\n",
    "\n",
    "# load the selected model/Network convolutional base weights\n",
    "\n",
    "input_shape = input_shape\n",
    "\n",
    "if Network_name == \"vgg16\":\n",
    "    base = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "elif Network_name == \"vgg19\":\n",
    "    base = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "elif Network_name == \"resnet50\":\n",
    "    base = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "elif Network_name == \"inception\":\n",
    "    base = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "elif Network_name == \"xception\":\n",
    "    base = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "else:\n",
    "    babe = None\n",
    "    \n",
    "print (\"[INFO] Successfully loaded the convolutional base of the\", Network_name, \"Pretrained Network\")\n",
    "print('{} {}'.format(\"Shown below is the convolutional base summary for: \", Network_name))\n",
    "\n",
    "print (\"[INFO] Below is the summary of the convolutional base of: \", Network_name)\n",
    "print(base.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a Custom Densely-Connected Classifier on Top of the Convolutional Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create our custom classification layer\n",
    "num_classes = 4    # change this number depending on the number of training classes\n",
    "\n",
    "# Create a custom Classifier Model \n",
    "x = base.output\n",
    "x = Flatten()(x)                       # Flatten convolutional base \n",
    "x = Dense(256, activation='relu')(x)   # add a dense layer with ReLu activation\n",
    "x = Dropout(0.5)(x)                    # add dropout\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # add a classification layer\n",
    "\n",
    "# the following is the model that will be trained\n",
    "model = Model(inputs=base.input, outputs=predictions)\n",
    "\n",
    "print(model.summary())                # print the new whole model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze the Convolutional Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze the convolutional base before compiling and training the model. This prevents the weights of the convolutional base from being updated during training, otherwise the representations previously learned by the convolutional base will be modified during training, i.e, freezing prevents the representations previously learned by the convolutional base from being modified during training. Since the dense layers added on top are randomly initialized, freezing prevents very large weight updates from being propagated through the network as this would destroy the previously learned representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check the effect of freezing the convolutional base on the number of trainable weights\n",
    "print('This is the number of trainable weights before freezing the convolutional base:', len(model.trainable_weights))\n",
    "\n",
    "# freeze all the layers except the dense (fully connected) layers\n",
    "for layer in base.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "print('This is the number of trainable weights after freezing the convolutional base:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can now check the trainable status of the individual layers\n",
    "# in the whole model\n",
    "for i, layer in enumerate(model.layers):\n",
    "   print(i, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile the model to ensure the 'trainable' changes have taken effect\n",
    "model.compile(optimizer=optimizers.Adam(lr=2e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check the number of trainable parameters \n",
    "# this should show in the summary after compiling the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "It is essential to train the model at this stage (before fine-tuning some layers in the base) because the top layers of the convolutional base can only be fine-tuned when the classifier on top has been trained, otherwise the representations previously learned by the layers being fine-tuned will be destroyed by the very large error signal that will be propagated (by the untrained classifier) through the network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time the training process\n",
    "%%time                                            \n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=ntotalTrain // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data= val_generator,\n",
    "                    validation_steps=ntotalVal // batch_size,\n",
    "                    class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model.save(\"C:\\\\Path\\\\to\\\\TrainedModel.h5\")\n",
    "\n",
    "# You can also save model without the optimizer to avoid the optimizer error shown when loading the trained model later or\n",
    "# in a different environment\n",
    "\n",
    "model.save(\"C:\\\\Path\\\\to\\\\TrainedModel_1.h5, include_optimizer= False\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the steps above are used for feature extraction with data augmentation type of transfer learning. The following steps can be added to turn this feature extraction transfer learning into a fine-tuning type of transfer learning. BUT we need to move the steps for saving the model and ploting the results to the end of the process. We do not need to save or plot the intermedairy resulsts. WE CAN also reduce the number of EPOCHS that we used in the feature extraction steps (just run for enough epochs to ensure that the top/classification layers are well trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model Performance/ Plot the results\n",
    "## IMPORTANT\n",
    "\n",
    "We can also evaluate the model performance tt this point if we want. This will essentially turn the code in this Jupyter Notebook into both a feature extractor (with data augmentation) and a fine-tuning model. This is because at this point, all the above steps are for feature extraction transfer learning. Combining the above steps with the steps that follow below wil give a final performance for fine-tuning the model. This kills two birds with one stone.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Training Results¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code will plot the loss and accuracy values during model training\n",
    "\n",
    "# Get values that were specified during model compilation which are saved in the \n",
    "# history object\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Get the number of epochs from the values in the 'acc' list\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Training and validation accuracy plot [Accuracy at each epoch]\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')       \n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')  \n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='center right')\n",
    "plt.figure()\n",
    "\n",
    "# Training and validation loss plot [Loss at each epoch]\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='center right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to determine the crop diseases were are training\n",
    "# split the BASE PATH into it's parts\n",
    "BASE_PATH_LIST = BASE_PATH.split(os.sep)\n",
    "print(BASE_PATH_LIST)\n",
    "CropName = BASE_PATH_LIST[-1] \n",
    "\n",
    "# extract the crop of interest from the path and print its name\n",
    "print('{}{}'.format(\"This network is learning to classify the diseases of the crop known as:..\", CropName)\n",
    "\n",
    "# list the target names (diseases being classified) for each crop in alphaneumeric order \n",
    "      \n",
    "if BASE_PATH_LIST[-1] == \"Apples\":\n",
    "      target_names = ['AppleScab', 'BlackRot', 'CedarAppleRust', 'Healthy']\n",
    "elif BASE_PATH_LIST[-1] == \"Grapes\":\n",
    "       target_names = ['Healthy', 'GrapeBlackMeasles', 'GrapeBlackRot', 'GrapeLeafBlight']      \n",
    "elif BASE_PATH_LIST[-1] == \"Tomatoes\": \n",
    "      target_names = ['Healthy', 'TomatoBacterialSpot', 'TomatoEarlyBlight', 'TomatoLateBlight']\n",
    "elif BASE_PATH_LIST[-1] == \"Corn\":\n",
    "      ['CornCommonRust', 'CornGrayLeafSpot', 'CornNorthernLeafBlight', 'Healthy'] \n",
    "else:\n",
    "      target_names = None\n",
    "      \n",
    "print('{}{}'.format(\"The current target names of the diseases are:..\", target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model on the Testing Dataset/ Make Inferences from Unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, reset the testing generator before using the trained model to make predictions\n",
    "# on the test data\n",
    "print(\"[INFO] Now evaluating the trained network...\")\n",
    "test_generator.reset()\n",
    "predictions = model.predict_generator(test_generator,\n",
    "                                   steps=(ntotalTest // batch_size) + 1) # +1 takes care of the remaining images if \n",
    "                                                                         # ntotalTest // batch_size is not a whole number\n",
    "# for every image in the testing set, find the the index of the label that has \n",
    "# the corresponding highest predicted probability\n",
    "pred_Idxs = np.argmax(predictions, axis=1)\n",
    "\n",
    "# set target names\n",
    "target_names = target_names\n",
    "\n",
    "\n",
    "# display a well-formated classification results\n",
    "print(classification_report(test_generator.classes, pred_Idxs,                            \n",
    "                            target_names = target_names)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Confusion Matrix\n",
    "\n",
    "CM = confusion_matrix(test_generator.classes, pred_Idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(CM, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.    \n",
    "    \"\"\"\n",
    "    \n",
    "  # plt.figure(figsize=[10,10]) # set the size of the confusion matrix if default is too small \n",
    "    plt.imshow(CM, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tickmarks = np.arange(len(classes))\n",
    "    plt.xticks(tickmarks, classes, rotation=45)\n",
    "    plt.yticks(tickmarks, classes)    \n",
    "        \n",
    "    if normalize:\n",
    "        CM = CM.astype('float') / CM.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(CM)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = CM.max() / 2.\n",
    "    for i, j in itertools.product(range(CM.shape[0]), range(CM.shape[1])):\n",
    "        plt.text(j, i, format(CM[i, j], fmt),\n",
    "                    horizontalalignment=\"center\", verticalalignment=\"center\",\n",
    "                    color=\"white\" if CM[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Confusion Matrix Without Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print network name and the crop name\n",
    "print('{}{}{}{}{}'.format(\"The Confusion Matrix below is for: \",CropName, \" trained on:\", Network_name, \" Network\"))\n",
    "print()   \n",
    "\n",
    "print(CropName,Network_name)\n",
    "print()\n",
    "\n",
    "# plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(CM, classes=target_names,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Normalized Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print network name and the crop name\n",
    "print('{}{}{}{}{}'.format(\"The Confusion Matrix below is for: \",CropName, \" trained on:\", Network_name, \" Network\"))\n",
    "print()   \n",
    "\n",
    "print(CropName,Network_name)\n",
    "print()\n",
    "\n",
    "# plot normalized confusion matrix\n",
    "plot_confusion_matrix(CM, classes=target_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"[INFO] Now evaluating the trained network...\")\n",
    "\n",
    "# summarize the evaluation of the model on test data\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=ntotalTest // batch_size)\n",
    "print('Test accuracy of the model is:', test_acc)\n",
    "print('Test loss of the model is:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfreeze the Convolutional Base and Freeze Some Layers Inside this Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this step, if for example we are fine-tuning a pre-trained VGG16 network, we may fine-tune the last 3 convolutional layers of the VGG16 network. This means that all the layers up to block4_pool will be frozen and the layers: block5_conv1, block5_conv2, and block5_conv3 will now be trainable. It is advisable to only fine-tune the top convolutional layers since these are the layers that encode more specialized features whereas the initial layers encode more generic features. We should not fine-tune many layers because the more the parameters we train, the more we RISK overfitting the model. This process should be repeated with other pre-trained networks. The number of layers to unfreeze may, for example, be selected to ensure that the number of unfrozen parameters is roughly the same for the different networks to ensure fair comparison of the networks' performances.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the top layers are well trained and we can start fine-tuning convolutional layers from VGG16/other pre-trained networks. We will freeze the bottom N layers\n",
    "and train the remaining top layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the code here lets us visualize layer names and layer indices to help us determine the number of layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now let's visualize layer names and layer indices in the whole model before unfreezing the conv base:\n",
    "for i, layer in enumerate(model.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, each layer has a parameter called trainable. To freeze the weights of a particular layer, the trainable parameter is set to False to indicate that that particular  layer should not be trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfreeze the Convolutional Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('This is the number of trainable weights before unfreezing the convolutional base:', len(model.trainable_weights))\n",
    "\n",
    "for layer in base.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "print('This is the number of trainable weights after unfreezing the convolutional base:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can now check the trainable status of the individual layers\n",
    "# in the whole model\n",
    "for i, layer in enumerate(model.layers):\n",
    "   print(i, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze Some Layers Inside the Convolutional Base\n",
    "Only higher layers of the network should be fine-tuned because they are the ones that encode the more specialized features of the custom problem. Fine-tuning lower layers is not only costly, it also increases the risk of overfitting because more parameters will be trained with the small custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"[REMINDER]The network we are working with is:  \", Network_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function determines the nnumber of layers to freeze for each different network\n",
    "# these layer numbers can be adjusted e.g., to ensure that the number of frozen layers \n",
    "# have roughly the same number of trainable parameters if our goal is to compare the performance of \n",
    "# different fine-tuned networks\n",
    "\n",
    "def freezer(Network_name):\n",
    "    if Network_name == \"vgg16\":\n",
    "        for layer in model.layers[:15]:   \n",
    "            layer.trainable = False       \n",
    "        for layer in model.layers[15:]:   \n",
    "            layer.trainable = True        # No. of parameters fine-tuned = 7,079,424\n",
    "            \n",
    "    elif Network_name == \"vgg19\":\n",
    "        for layer in model.layers[:18]:   \n",
    "            layer.trainable = False\n",
    "        for layer in model.layers[18:]:   \n",
    "            layer.trainable = True        # No. of parameters fine-tuned = 7,079,424\n",
    "\n",
    "    elif Network_name == \"resnet50\":\n",
    "        for layer in model.layers[:157]:   \n",
    "            layer.trainable = False\n",
    "        for layer in model.layers[157:]:   \n",
    "            layer.trainable = True         # No. of parameters fine-tuned = 7,892,480\n",
    "\n",
    "    elif Network_name == \"inception\":\n",
    "        for layer in model.layers[:261]:  \n",
    "            layer.trainable = False\n",
    "        for layer in model.layers[261:]:  \n",
    "            layer.trainable = True        # No. of parameters fine-tuned = 7,216,704\n",
    "\n",
    "    elif Network_name == \"xception\":\n",
    "        for layer in model.layers[:113]:  \n",
    "            layer.trainable = False\n",
    "        for layer in model.layers[113:]:  \n",
    "            layer.trainable = True        # No. of parameters fine-tuned = 7,340,552\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('The number of trainable weights before freezing the last 4 (N) layers in the convolutional base is:', len(model.trainable_weights))\n",
    "\n",
    "freezer(Network_name)\n",
    "\n",
    "print('And the number of trainable weights after freezing the the last 4 layers in the convolutional base is:', len(model.trainable_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can now check the trainable status of the individual layers\n",
    "# in the convolutional base\n",
    "for layer in base.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's visualize trainable layers names and layer indices in the whole model after freezing some layers in the conv base:\n",
    "for i, layer in enumerate(model.layers):\n",
    "   print(i, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Compile the model\n",
    "\n",
    "To avoid errors, recompile the model before printing the summary for the 'trainable' changes to take effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile the model to ensure the 'trainable' changes have taken effect\n",
    "# the model should always be re-compiled whenever weight trainability is changed after\n",
    "# the initial compilation, otherwise the changes will be ignored\n",
    "# here we can use a lower learning rate than the one used in the earlier training\n",
    "# using a lower learning rate limits the size of the changes made to the representations \n",
    "# of the layers being fine-tuned because large updates will harm the representations \n",
    "# already learned\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-5),  # now use a lower learning rate\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now check the number of trainable parameters (for fine tuning)\n",
    "# we can use this to ensure roughly the same number of parameters are fine-tuned when we compare networks\n",
    "# \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the batch size\n",
    "batch_size = 30\n",
    "\n",
    "# set the number of epochs\n",
    "epochs = 15                   # increase the number of epochs for the fine-tuning process\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time # optionallt time the training process\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=ntotalTrain // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data= val_generator,\n",
    "                    validation_steps=ntotalVal // batch_size,\n",
    "                    class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"C:\\\\Path\\\\to\\\\TrainedModel3.h5\")\n",
    "\n",
    "# You can also save model without the optimizer to avoid the optimizer error shown when loading the trained model later or\n",
    "# in a different environment\n",
    "\n",
    "model.save(\"C:\\\\Path\\\\to\\\\TrainedModel_4.h5, include_optimizer= False\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code will plot the curves of loss and accuracy during training\n",
    "\n",
    "# Get values that were specified during model compilation which are saved in the \n",
    "# history object\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Get the number of epochs from the values in the 'acc' list\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Training and validation accuracy plot [Accuracy at each epoch]\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')       \n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')  \n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='center right')\n",
    "plt.figure()\n",
    "\n",
    "# Training and validation loss plot [Loss at each epoch]\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='center right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model on the Testing Dataset/ Make Inferences from Unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, reset the testing generator before using the trained model to make predictions\n",
    "# on the test data\n",
    "print(\"[INFO] Now evaluating the trained network...\")\n",
    "test_generator.reset()\n",
    "predictions = model.predict_generator(test_generator,\n",
    "                                   steps=(ntotalTest // batch_size) + 1)  \n",
    "                                                                         \n",
    "# for every image in the testing set, find the the index of the label that has \n",
    "# the corresponding highest predicted probability\n",
    "pred_Idxs = np.argmax(predictions, axis=1) # get the highest prediction indices for each sample \n",
    "\n",
    "# set target names\n",
    "target_names = target_names\n",
    "\n",
    "# print a well-formated classification results\n",
    "print(classification_report(test_generator.classes, pred_Idxs,                            \n",
    "                            target_names = target_names)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Confusion Matrix\n",
    "\n",
    "CM = confusion_matrix(test_generator.classes, pred_Idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(CM, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.    \n",
    "    \"\"\"\n",
    "    \n",
    "  # plt.figure(figsize=[10,10]) # set the size of the confusion matrix if default is too small\n",
    "    plt.imshow(CM, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tickmarks = np.arange(len(classes))\n",
    "    plt.xticks(tickmarks, classes, rotation=45)\n",
    "    plt.yticks(tickmarks, classes)    \n",
    "        \n",
    "    if normalize:\n",
    "        CM = CM.astype('float') / CM.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(CM)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = CM.max() / 2.\n",
    "    for i, j in itertools.product(range(CM.shape[0]), range(CM.shape[1])):\n",
    "        plt.text(j, i, format(CM[i, j], fmt),\n",
    "                    horizontalalignment=\"center\", verticalalignment=\"center\",\n",
    "                    color=\"white\" if CM[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Confusion Matrix Without Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print network name and the crop name\n",
    "print('{}{}{}{}{}'.format(\"The Confusion Matrix below is for: \",CropName, \" trained on:\", Network_name, \" Network\"))\n",
    "print()   \n",
    "\n",
    "print(CropName,Network_name)\n",
    "print()\n",
    "\n",
    "# plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(CM, classes=target_names,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Normalized Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print network name and the crop name\n",
    "print('{}{}{}{}{}'.format(\"The Confusion Matrix below is for: \",CropName, \" trained on:\", Network_name, \" Network\"))\n",
    "print()   \n",
    "\n",
    "print(CropName,Network_name)\n",
    "print()\n",
    "\n",
    "# plot normalized confusion matrix\n",
    "plot_confusion_matrix(CM, classes=target_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"[INFO] Now evaluating the trained network...\")\n",
    "\n",
    "# summarize the evaluation of the model on test data\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=ntotalTest // batch_size)\n",
    "print('Test accuracy of the model is:', test_acc)\n",
    "print('Test loss of the model is:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the following code uses the test generator to visualize the errors made in the testing set\n",
    " \n",
    "# get the filenames from the generator\n",
    "fnames = test_generator.filenames\n",
    " \n",
    "# get the ground truth from generator\n",
    "ground_truth = test_generator.classes\n",
    " \n",
    "# get the label to class mapping from the generator\n",
    "label2index = test_generator.class_indices\n",
    " \n",
    "# now get the mapping from class index to class label\n",
    "idx2label = dict((v,k) for k,v in label2index.items())\n",
    " \n",
    "\n",
    "#predicted_classes = np.argmax(predictions,axis=1) \n",
    "errors = np.where(pred_Idxs != ground_truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors),test_generator.samples))\n",
    " \n",
    "# Show the errors\n",
    "for i in range(len(errors)):\n",
    "    pred_class = np.argmax(pred_Idxs[errors[i]])\n",
    "    pred_label = idx2label[pred_class]\n",
    "     \n",
    "    title = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(\n",
    "        fnames[errors[i]].split('/')[0],\n",
    "        pred_label,\n",
    "        pred_Idxs[errors[i]][pred_class])\n",
    "     \n",
    "    original = load_img('{}/{}'.format(TEST_PATH, fnames[errors[i]]))\n",
    "    plt.figure(figsize=[7,7])\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.imshow(original)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlock the Kernel Lock on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you execute the notebook and leave it running (especially when working with multiple notebooks),\n",
    "# the following code will release the kernel lock on the GPU and help avoid the raising of \"resource exhausted\"\n",
    "# errors\n",
    "\n",
    "%%javascript \n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DeepLearning]",
   "language": "python",
   "name": "conda-env-DeepLearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
